easy_joby_py/
    README.md
    config.py
    requirements.txt
    app.py
    Documentation.md
    src/
        templates/
            agent_summary_reconstruction_code.html
            directory_structure.html
            index.html
            ollama.html
            error.html
            split_results.html
            project_documentation.html
            image_description.html
            refactor.html
            split_file.html
        routes/
            home.py
            directory_structure.py
            file_splitter.py
            agent_summary_reconstruction.py
            refactor.py
            project_documentation.py
            image_description.py
            ollama_response.py
        modules/
            concatenate_project.py
            agent_summary_reconstruction_code.py
            directory_structure.py
            refactor.py
            gpt.py
            project_documentation.py
            image_description.py
            file_processor.py
        static/
            images/
            css/
                styles.css</br>

/home/cleudeir/Pessoal/easy_joby_py/README.md:


## Project structure
```                    
easy_joby_py/
    README.md
    config.py
    requirements.txt
    app.py
    Documentation.md
    src/
        templates/
            agent_summary_reconstruction_code.html
            directory_structure.html
            index.html
            ollama.html
            error.html
            split_results.html
            project_documentation.html
            image_description.html
            refactor.html
            split_file.html
        routes/
            home.py
            directory_structure.py
            file_splitter.py
            agent_summary_reconstruction.py
            refactor.py
            project_documentation.py
            image_description.py
            ollama_response.py
        modules/
            agent_summary_reconstruction_code.py
            directory_structure.py
            refactor.py
            gpt.py
            project_documentation.py
            image_description.py
            file_processor.py
        static/
            images/                
            css/
                styles.css                
```
## Descrição do Projeto

Este projeto é uma aplicação web para gerenciamento de projetos, oferecendo diversas ferramentas para otimizar o processo de desenvolvimento.  Ele inclui funcionalidades para: obter estrutura de diretórios, interagir com sistemas externos, dividir arquivos, refatorar código, gerar documentação de imagens, reconstrução de resumos de agentes e gerar documentação de projetos.  Utiliza modelos de linguagem grandes (LLMs) como Ollama e Gemini para auxiliar na geração de documentação.

## Dependências

Antes de usar este projeto, instale as dependências:

```
flask
pdfplumber
scikit-learn
jinja2
pytesseract
torch
torchvision
pillow
opencv-python
flasgger
python-docx
ollama
google-generativeai
markdown
python-dotenv
pypandoc
```

## Como Instalar

1. Clone este repositório.
2. Instale as dependências: `pip install -r requirements.txt`
3. Crie um arquivo `.env` com as configurações necessárias (chave de API do Google Generative AI, etc.).
4. Execute a aplicação: `python app.py` (ou comando apropriado para o seu ambiente de desenvolvimento).

## Como Usar

A aplicação web fornece uma interface para acessar diferentes módulos. Cada módulo oferece funcionalidades específicas:

* **Estrutura de diretórios:** Exibe a estrutura de arquivos e pastas de um diretório especificado.
* **Divisão de arquivos:** Permite dividir arquivos grandes em partes menores, com base em diferentes critérios (texto, número de linhas, parágrafos).
* **Refatoração de código:** Ajuda a refatorar código, separando-o em funções.
* **Documentação de imagens:** Gera documentação descritiva para imagens, utilizando modelos de linguagem.
* **Reconstrução de resumos de agentes:** Reconstrói o código baseado em um resumo gerado por um modelo de linguagem.
* **Documentação de projetos:** Gera uma documentação completa para um projeto, incluindo resumos de arquivos, sumário geral e post para o LinkedIn.
* **Interface Ollama:** Permite interação com modelos Ollama.


## Arquitetura

A aplicação é baseada em Flask, utilizando blueprints para organizar as rotas.  Os módulos individuais (estrutura de diretórios, divisão de arquivos, etc.) são implementados como blueprints separados, melhorando a organização e manutenibilidade do código. A geração de documentação se baseia fortemente em modelos de linguagem grandes acessados através de APIs (Ollama e Google Generative AI).


## Pipeline

O pipeline geral envolve:

1. **Solicitação do Usuário:** O usuário interage com a aplicação web via formulários.
2. **Processamento:** A aplicação processa a solicitação do usuário, possivelmente utilizando modelos de linguagem para gerar documentação ou realizar outras tarefas.
3. **Geração de Saída:** A aplicação gera a saída, que pode ser um HTML, um arquivo de texto ou outro formato, dependendo da funcionalidade.
4. **Exibição da Saída:** A saída é exibida ao usuário na interface web.

A arquitetura modular permite que cada funcionalidade tenha seu próprio pipeline específico, mas seguindo o fluxo geral descrito acima.
                
                
/home/cleudeir/Pessoal/easy_joby_py/config.py:

class Config:
    UPLOAD_FOLDER = 'uploads'
    DEBUG = True
    SWAGGER = {
        'title': 'My Flask API',
        'uiversion': 3
    }

/home/cleudeir/Pessoal/easy_joby_py/requirements.txt:

flask
flasgger
pdfplumber
python-docx
ollama
markdown
google-generativeai
python-dotenv
scikit-learn
python-docx
jinja2 
pypandoc
pytesseract
torch 
torchvision 
pillow 
opencv-python

/home/cleudeir/Pessoal/easy_joby_py/app.py:

from flask import Flask
from config import Config
from src.routes import register_blueprints  # Import the blueprint registration function

app = Flask(__name__, template_folder='src/templates', static_folder='src/static')
app.config.from_object(Config)

# Register all blueprints
register_blueprints(app)

if __name__ == '__main__':
    app.run(debug=app.config['DEBUG'])

/home/cleudeir/Pessoal/easy_joby_py/Documentation.md:

# Summary: config.py

## Purpose of the Code

* This code defines a configuration class for a Flask application. It sets up various settings for the application, including the upload folder, debug mode, and Swagger configuration.

## Business Rule

* This code defines the configuration parameters necessary for the Flask application to function correctly. It sets the upload folder location, enables debug mode, and configures Swagger for API documentation. 

## List External Libraries

* No libraries used.

---

# Summary: requirements.txt

## Purpose of the Code

* The code likely involves web development, document processing, and potentially AI integration. 

##  Business Rule

* No business rule is mentioned.

## List external libraries


* **flask:** A popular Python framework for building web applications.
* **flasgger:** A library for automatically generating API documentation using Swagger.
* **pdfplumber:** A library for extracting text and tables from PDF documents.
* **python-docx:**  A library for creating and manipulating Microsoft Word documents.
* **ollama:** An open-source large language model framework.
* **markdown:** A lightweight markup language for creating formatted text.
* **google-generativeai:**  Google's generative AI library for tasks like text generation and image creation.
* **python-dotenv:** A library for loading environment variables from a `.env` file. 

---

# Summary: app.py

## Purpose of the Code

* This code creates a Flask application that serves both HTML routes and API endpoints. 

## Business Rule

* The code sets up the Flask application, loads configurations from the 'Config' object, initializes Swagger UI for documentation, and registers blueprints for HTML and API routes. 

## List external libraries

* `flask`: A Python framework for building web applications.
* `flasgger`: A Flask extension for generating API documentation with Swagger UI.
* `config`: A module containing configuration settings for the application.
* `src.routes.html_routes`: A module containing routes for HTML pages.
* `src.routes.api_routes`: A module containing routes for API endpoints. 

---

# Summary: directory_structure.html

## Purpose of the Code

* This code displays a web page that allows users to obtain the directory structure of a specified directory path.


##  Business rule

* The page either displays a form to input the directory path or displays the retrieved directory structure in a preformatted text block, depending on whether a valid path has been provided.

## List external libraries

* **Jinja2**: A templating engine used for rendering the HTML with dynamic content (e.g., displaying the directory structure).
* **Flask**: A Python web framework used to handle the routing and processing of user requests.

---

# Summary: index.html

## Purpose of the Code

* This code creates a simple HTML dashboard for a project manager application.

## Business Rule

* The dashboard presents a list of available modules, each with a link to a specific route.

## List External Libraries

* No libraries used. 

---

# Summary: ollama.html

## Purpose of the Code

* This code provides a web interface to interact with an Ollama model. Users can select a model, provide system and user prompts, and receive a response. The response is displayed in a preformatted area.

##  bussiness rule

* The code utilizes local storage to save the system prompt, selected model, and user prompt so that they persist across sessions.

## List external libraries

* No libraries used. 

---

# Summary: split_results.html

## Purpose of the Code


* This code is a webpage that displays the results of splitting a file by a given delimiter. 
##  bussiness rule

    
*  The code retrieves the sections of a file that were split by a specific text delimiter, and displays them in a list. 
## List external libraries


    * `url_for`: This is a function from the Flask framework for creating URLs.
    * `static`: This is a function from the Flask framework for serving static files.
    * `css/styles.css`: This is a stylesheet for styling the webpage. 

---

# Summary: project_documentation.html

## Purpose of the Code

* This code provides a web interface to generate documentation for projects. 
* Users can enter a project directory path and select a GPT provider and model to generate documentation. 

## Business Rule

* The code leverages external GPT providers to analyze the project's code and generate documentation. 
* It presents a user-friendly form for selecting project paths and providers. 

## List external libraries

* **Jinja2:** A templating language for generating dynamic HTML content.
* **Flask:** A lightweight web framework for creating web applications.
* **GPT Providers:** (Ollama, Gemini) -  These providers are external APIs that generate text based on given prompts.

---

# Summary: split_file.html

## Purpose of the Code

* This code implements a web form that allows users to upload a file and split it based on a chosen method.

##  bussiness rule

* The code provides three options for splitting: 
    * "Split by Text": Splits the file based on a user-provided text string.
    * "Split by Line Number": Splits the file based on a user-provided line number.
    * "Split by Paragraphs": Splits the file based on paragraph breaks.

## List external libraries

* No libraries used.

---

# Summary: api_routes.py

## Purpose of the Code

* The code defines a Flask Blueprint named `api_routes` that exposes several API endpoints for various tasks related to project documentation, directory structure analysis, file splitting, and interactions with the Ollama language model. 

## Business Rule

*  The code implements several business rules, including:
    * Generation of project documentation using a selected LLM model (Ollama) and GPT provider. 
    * Fetching the directory structure of a given directory path. 
    * Splitting files based on provided text, lines, or paragraphs.
    * Interacting with the Ollama language model to generate responses based on user prompts.
## List external libraries

* **Flask:** Framework for building web applications
* **flasgger:**  Used to generate Swagger documentation for APIs
* **os:**  Used for interacting with the operating system (e.g., creating directories)
* **jsonify:**  Flask extension for serializing Python data structures to JSON
* **request:** Flask extension for accessing incoming request data
* **current_app:** Flask object representing the current application
* **src.modules.directory_structure:**  Module containing functions related to directory structure analysis
* **src.modules.gpt:** Module containing functions for interacting with the Ollama language model
* **src.modules.file_processor:** Module containing functions for reading and splitting files
* **src.modules.project_documentation:** Module for generating project documentation. 

---

# Summary: html_routes.py

## Purpose of the Code

* The code is a Flask application that provides several functionalities related to file processing, documentation generation, and interaction with an LLM (Large Language Model) like Ollama.

## Business Rule

* The application allows users to upload files, split them into sections based on different criteria (text, lines, paragraphs), generate project documentation using an LLM, and summarize the documentation using the LLM. 
* It also provides functionality to interact with Ollama and get responses based on user prompts.

## List external libraries

* **Flask:**  A web framework for Python.
* **markdown:** A library for converting Markdown text to HTML.
* **os:** A Python module providing access to operating system functionalities.
* **src.modules.directory_structure:** A module containing functions to retrieve directory structure.
* **src.modules.gpt:** A module containing functions to interact with Ollama, including fetching available models and getting responses.
* **src.modules.file_processor:**  A module containing functions to read different file types (PDF, DOCX, TXT) and split them into sections.
* **src.modules.project_documentation:** A module containing functions to generate project documentation, summarize it using an LLM, and create LinkedIn posts based on the summary. 

---

# Summary: directory_structure.py

## Purpose of the Code

* The code generates a textual representation of the directory structure of a given path, excluding hidden directories (those starting with a dot).

## Business Rule

*  Ignore hidden directories, presenting only the structure of visible directories and their contained files.

## List external libraries

* `os`: This library provides functions for interacting with the operating system, including file and directory manipulation. 

---

# Summary: gpt.py

## Purpose of the Code

* The code provides functions for interacting with Google Generative AI (GenAI) and Ollama models. It enables sending user prompts and retrieving responses from both platforms. 

## Business Rule

*  The code is intended for applications requiring interaction with large language models (LLMs) like Google's Gemini and Ollama models. It provides flexibility in choosing the LLM based on the requirements of the application.
## List external libraries

* **google.generativeai as genai**: Google Generative AI library for interacting with Google's LLMs.
* **ollama**: Ollama library for communicating with Ollama's LLMs. 
* **os**: Python's operating system library for environment variable access. 

---

# Summary: project_documentation.py

## Purpose of the Code

* The code summarizes files in a project directory using a chosen language model (Ollama or Gemini). It generates documentation by creating summaries of individual files and saving them to a project documentation file.

## Business Rule

* The code is designed to automatically generate documentation for a project by summarizing the content of each file. The summaries are generated using an external LLM provider and are then combined into a single documentation file.

## List external libraries

* **os:** Provides functions for interacting with the operating system, including file operations.
* **time:** Provides time-related functions, including the sleep function used to introduce a delay between summaries.
* **src.modules.file_processor:**  Contains functions for reading PDF and DOCX files.
* **src.modules.gpt:** Contains functions for interacting with the selected LLM (Ollama or Gemini).
* **fnmatch:** Provides functions for Unix-style filename matching using patterns. 

---

# Summary: file_processor.py

## Purpose of the Code

* This code defines functions to read and split text from files, specifically PDF, DOCX and TXT files. 

## Business Rule

*  There are no explicit business rules defined in this code.

## List external libraries

* **pdfplumber:** Used for extracting text from PDF files while preserving formatting.
* **docx:**  Used for extracting text from DOCX files. 

---

# Summary: styles.css

## Purpose of the Code

* This code provides CSS styles for a web application. 

##  bussiness rule

* Not applicable.
## List external libraries

* No libraries used. 

---

# Summary: split_file.yml

## Purpose of the Code

* This code provides an API endpoint for splitting files into sections based on user-defined criteria.

## Business Rule

* Users can upload a file and choose a splitting method: "text" (split by a specific text pattern), "lines" (split by a specified number of lines), or "paragraphs". 
* The code processes the file and returns an array of file sections along with a description of the splitting method and value used.

## List external libraries

* No libraries used. 

---

# Summary: ollama.yml

## Purpose of the Code

* This code defines an API endpoint for generating responses using the Ollama AI model.
##  bussiness rule

* The API takes a user prompt and system-level prompt as input, selects an AI model, and uses Ollama to generate a response.
## List external libraries


* No libraries used. 

---

# Summary: directory_structure.yml

## Purpose of the Code

This code retrieves the structure of a directory given a path. 

## Business Rule

The code analyzes a given directory path and returns a nested dictionary structure representing the directory's contents. 

## List external libraries

* No libraries used. 

---

# Summary: get_project_documentation.yml

## Purpose of the Code

* The code defines an API endpoint for generating project documentation. 

##  bussiness rule

*  The API accepts a project path and a documentation model as input and generates a documentation string based on these parameters.

## List external libraries

* No libraries used. 

---

/home/cleudeir/Pessoal/easy_joby_py/src/templates/agent_summary_reconstruction_code.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Documentation</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Generate Project Documentation</h1>

        <form id="documentationForm" method="POST">
            <label for="file">Choose a file:</label>
            <input type="file" id="file" name="file" required>

            <label for="gpt_provider">GPT Provider:</label>
            <select id="gpt_provider" name="gpt_provider" required onchange="toggleModelDropdown()">
                <option value="ollama">Ollama</option>
                <option value="gemini">Gemini</option>
            </select>

            <div id="modelSelection" style="display: none;">
                <label for="model">Model:</label>
                <select id="model" name="model">
                    {% for model in models %}
                    <option value="{{ model }}">{{ model }}</option>
                    {% endfor %}
                </select>
            </div>
            <button type="submit" id="generateDocumentationButton">Generate Documentation</button>
        </form>

        <!-- Documentation Content Section -->
        <div id="documentationContent" style="display: none;">
            <h2>Generated Documentation:</h2>
            <div id="documentationOutput">
                <!-- Streamed content will be added here -->
            </div>
        </div>

        <!-- Loading Overlay -->
        <div class="loading-overlay" id="loadingOverlay">
            <div id="loading">Generation...</div>
        </div>
        <br>
        <a href="{{ url_for('home_routes.home') }}" style="text-align: center; display: block;">Back to Dashboard</a>
    </div>

    <script>
        window.addEventListener('DOMContentLoaded', function () {
            // Load the remembered values from localStorage if available

            const savedGptProvider = localStorage.getItem('gpt_provider');

            const savedModel = localStorage.getItem('model');

            if (savedGptProvider) {
                document.getElementById('gpt_provider').value = savedGptProvider;
                // To show/hide model dropdown based on the provider
            }
            if (savedModel) {
                document.getElementById('model').value = savedModel;
            }

            toggleModelDropdown();
        });

        // Display loading overlay and handle form submission
        document.getElementById('documentationForm').addEventListener('submit', function (event) {
            event.preventDefault();
            // Save the form values to localStorage        
            localStorage.setItem('gpt_provider', document.getElementById('gpt_provider').value);
            localStorage.setItem('model', document.getElementById('model').value);

            document.getElementById('loadingOverlay').style.display = 'flex';
            document.getElementById('documentationOutput').innerHTML = '';

            // Fetch streaming data from the server
            console.log('this.action: ', this.action);
            fetch(this.action, {
                method: 'POST',
                body: new FormData(this),
            })
                .then(response => {
                    // Ensure response body can be read as text for streaming
                    const reader = response.body.getReader();
                    const decoder = new TextDecoder("utf-8");

                    // Function to process each chunk of streamed data
                    function readStream() {
                        return reader.read().then(({ done, value }) => {
                            if (done) {
                                document.getElementById('loadingOverlay').style.display = 'none';
                                return
                            }

                            document.getElementById('documentationContent').style.display = 'block';

                            document.getElementById('loadingOverlay').style.display = 'none';

                            // Append each streamed chunk to the documentation output
                            const chunk = decoder.decode(value, { stream: true });
                            const div = document.createElement('div');
                            if (!chunk.includes("id='agent_coder'")) {
                                div.classList.add('markdown-content');  // Add a class for styling (optional)
                            }
                            div.innerHTML = chunk;  // Set the chunk content
                            // Append the div containing the chunk to the markdown content
                            document.getElementById('documentationOutput').appendChild(div);
                            document.getElementById('loadingOverlay').style.display = 'flex';
                            return readStream();
                        });
                    }
                    return readStream();
                })
                .catch(error => {
                    document.getElementById('documentationOutput').innerHTML = `<p>Error: ${error.message}</p>`;
                    document.getElementById('loadingOverlay').style.display = 'none';
                });
        });

        function toggleModelDropdown() {
            const provider = document.getElementById('gpt_provider').value;
            console.log('provider: ', provider);
            document.getElementById('modelSelection').style.display = provider === 'ollama' ? 'block' : 'none';
        }
        // Show the loading indicator
        function showLoading() {
            document.getElementById('loading').style.display = 'block';  // Show loading indicator
            document.getElementById('response').style.display = 'none';  // Hide response during loading
        }
    </script>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/directory_structure.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Directory Structure</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Get Directory Structure</h1>

        {% if structure %}
        <h2>Directory Structure:</h2>
        <pre>{{ structure }}</pre> <!-- Display the structure in a preformatted text block -->
        {% else %}
        <form method="POST">
            <label for="directory_path">Enter Absolute Directory Path:</label>
            <input type="text" id="directory_path" name="directory_path" placeholder="e.g. /path/to/directory" required>
            <br>
            <button type="submit">Get Structure</button>
        </form>
        {% endif %}

        <br>
        <a href="{{ url_for('home_routes.home') }}">Back to Dashboard</a>
    </div>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/index.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Manager</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Project Manager Dashboard</h1>
        <h2>Available Modules:</h2>
        <ul>
            <li><a href="{{ url_for('directory_structure_routes.get_directory_structure_route') }}">Get Directory
                    Structure</a></li>
            <li><a href="{{ url_for('ollama_response_routes.ollama_response') }}">Interact with Ollama</a></li>
            <li><a href="{{ url_for('file_splitter_routes.file_splitter') }}">Split a File</a></li>

            <li><a href="{{ url_for('refactor_routes.refactor') }}">Refactor</a></li>
            <li><a href="{{ url_for('image_description_routes.image_description') }}">Generate Image Documentation</a>
            </li>

            <li><a href="{{ url_for('agent_summary_reconstruction_routes.agent_summary_reconstruction_code') }}">Agent
                    Summary and
                    Reconstruction</a></li>

            <li><a href="{{ url_for('project_documentation_routes.get_project_documentation') }}">Generate
                    Project
                    Documentation</a></li>
        </ul>
    </div>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/ollama.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ollama Model Interaction</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
    <script>
        // Save the system prompt and selected model to local storage
        function saveInputs() {
            const systemPromptInput = document.getElementById('system_prompt');
            const modelSelect = document.getElementById('model');
            const userPromptInput = document.getElementById('user_prompt');

            localStorage.setItem('system_prompt', systemPromptInput.value);
            localStorage.setItem('selected_model', modelSelect.value);
            localStorage.setItem('user_prompt', userPromptInput.value);
        }

        // Load the system prompt and selected model from local storage when the page loads
        function loadInputs() {
            const savedPrompt = localStorage.getItem('system_prompt');
            const savedModel = localStorage.getItem('selected_model');
            const savedUserPrompt = localStorage.getItem('user_prompt');

            if (savedPrompt) {
                document.getElementById('system_prompt').value = savedPrompt;
            }

            if (savedModel) {
                document.getElementById('model').value = savedModel;
            }

            if (savedUserPrompt) {
                document.getElementById('user_prompt').value = savedUserPrompt;
            }
        }

        // Show the loading indicator
        function showLoading() {
            document.getElementById('loading').style.display = 'block';  // Show loading indicator
            document.getElementById('response').style.display = 'none';  // Hide response during loading
        }

        // Call load function when the page is loaded
        window.onload = loadInputs;
    </script>
</head>

<body>
    <div class="container">
        <h1>Interact with Ollama</h1>

        <form method="POST" onsubmit="showLoading(); saveInputs();">
            <label for="model">Choose Model:</label>
            <select id="model" name="model" required onchange="saveInputs()">
                {% for model in models %}
                <option value="{{ model }}">{{ model }}</option>
                {% endfor %}
            </select>

            <label for="system_prompt">System Prompt:</label>
            <textarea id="system_prompt" name="system_prompt" placeholder="Enter system prompt" required rows="4"
                oninput="saveInputs()"></textarea>

            <label for="user_prompt">User Prompt:</label>
            <textarea id="user_prompt" name="user_prompt" placeholder="Enter user prompt" required rows="4"
                oninput="saveInputs()"></textarea>

            <button type="submit">Get Response</button>
        </form>

        <!-- Loading indicator (initially hidden) -->
        <div id="loading" style="display:none; text-align: center; font-size: 1.5em; color: #3498db;">
            Processing... Please wait.
        </div>

        <!-- Response will be displayed here (hidden initially during loading) -->
        <div id="response" style="display: block;">
            {% if response %}
            <h2>Response:</h2>
            <pre>{{ response | safe }}</pre>
            {% endif %}
        </div>

        <br>
        <a href="{{ url_for('home_routes.home') }}">Back to Dashboard</a>
    </div>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/error.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Error</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Error</h1>
        <p>{{ message }}</p>
        <a href="{{ url_for('home_routes.home') }}">Go Back</a>
    </div>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/split_results.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Split Results</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Results of Splitting by "{{ split_text }}"</h1>

        <h2>Sections:</h2>
        <ul>
            {% for section in sections %}
            <li>
                <pre>{{ section }}</pre>
            </li>
            {% endfor %}
        </ul>

        <br>
        <a href="{{ url_for('file_splitter_routes.file_splitter') }}">Back to Split Another File</a>
    </div>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/project_documentation.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Documentation</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Generate Project Documentation</h1>

        <form id="documentationForm" method="POST">
            <label for="project_path">Project Directory Path:</label>
            <input type="text" id="project_path" name="project_path" placeholder="e.g. /path/to/project" required>

            <label for="gpt_provider">GPT Provider:</label>
            <select id="gpt_provider" name="gpt_provider" required onchange="toggleModelDropdown()">
                <option value="gemini">Gemini</option>
                <option value="ollama">Ollama</option>
            </select>

            <div id="modelSelection" style="display: none;">
                <label for="model">Model:</label>
                <select id="model" name="model">
                    {% for model in models %}
                    <option value="{{ model }}">{{ model }}</option>
                    {% endfor %}
                </select>
            </div>

            <button type="submit" id="generateDocumentationButton">Generate Documentation</button>
        </form>

        <!-- Documentation Content Section -->
        <div id="documentationContent" style="display: none;">
            <h2>Generated Documentation:</h2>
            <div id="documentationOutput">
                <!-- Streamed content will be added here -->
            </div>
        </div>

        <!-- Loading Overlay -->
        <div class="loading-overlay" id="loadingOverlay">
            <div id="loading">Generation...</div>
        </div>
        <br>
        <a href="{{ url_for('home_routes.home') }}" style="text-align: center; display: block;">Back to Dashboard</a>
    </div>

    <script>
        // Remember the previously entered values for project_path, gpt_provider, and model
        window.addEventListener('DOMContentLoaded', function () {
            // Load the remembered values from localStorage if available
            const savedProjectPath = localStorage.getItem('project_path');
            const savedGptProvider = localStorage.getItem('gpt_provider');
            const savedModel = localStorage.getItem('model');

            if (savedProjectPath) {
                document.getElementById('project_path').value = savedProjectPath;
            }

            if (savedGptProvider) {
                document.getElementById('gpt_provider').value = savedGptProvider;
                // To show/hide model dropdown based on the provider
            }

            if (savedModel) {
                document.getElementById('model').value = savedModel;
            }
            toggleModelDropdown();
        });

        // Display loading overlay and handle form submission
        document.getElementById('documentationForm').addEventListener('submit', function (event) {
            event.preventDefault();

            // Save the form values to localStorage
            localStorage.setItem('project_path', document.getElementById('project_path').value);
            localStorage.setItem('gpt_provider', document.getElementById('gpt_provider').value);
            localStorage.setItem('model', document.getElementById('model').value);

            document.getElementById('loadingOverlay').style.display = 'flex';
            document.getElementById('documentationOutput').innerHTML = '';

            // Fetch streaming data from the server
            fetch(this.action, {
                method: 'POST',
                body: new FormData(this),
            })
                .then(response => {
                    // Ensure response body can be read as text for streaming
                    const reader = response.body.getReader();
                    const decoder = new TextDecoder("utf-8");

                    // Function to process each chunk of streamed data
                    function readStream() {
                        return reader.read().then(({ done, value }) => {
                            if (done) {
                                document.getElementById('loadingOverlay').style.display = 'none';
                                return
                            }

                            document.getElementById('documentationContent').style.display = 'block';

                            document.getElementById('loadingOverlay').style.display = 'none';

                            // Append each streamed chunk to the documentation output
                            const chunk = decoder.decode(value, { stream: true });
                            const div = document.createElement('div');  // Create a new div for each chunk
                            div.classList.add('markdown-content');  // Add a class for styling (optional)
                            div.innerHTML = chunk;  // Set the chunk content
                            // Append the div containing the chunk to the markdown content
                            document.getElementById('documentationOutput').appendChild(div);
                            document.getElementById('loadingOverlay').style.display = 'flex';
                            document.getElementById('loadingOverlay').scrollIntoView({ behavior: "smooth", block: "end", inline: "nearest" });
                            return readStream();
                        });
                    }
                    return readStream();
                })
                .catch(error => {
                    document.getElementById('documentationOutput').innerHTML = `<p>Error: ${error.message}</p>`;
                    document.getElementById('loadingOverlay').style.display = 'none';
                });
        });

        function toggleModelDropdown() {
            const provider = document.getElementById('gpt_provider').value;
            document.getElementById('modelSelection').style.display = provider === 'ollama' ? 'block' : 'none';
        }
    </script>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/image_description.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Description Documentation</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Generate Image Documentation</h1>
        <form id="imageForm">
            <label for="path">Image Directory Path:</label>
            <input type="text" id="path" name="path" required>

            <label for="provider">Provider:</label>
            <select id="provider" name="provider" required onchange="toggleModelDropdown()">
                <option value="gemini">Gemini</option>
                <option value="ollama">Ollama</option>
            </select>

            <div id="modelSelection" style="display: none;">
                <label for="model">Model:</label>
                <select id="model" name="model">
                    {% for model in models %}
                    <option value="{{ model }}">{{ model }}</option>
                    {% endfor %}
                </select>
            </div>
            <div>
                <input type="checkbox" id="useCache" name="useCache" checked>
                Use cache
            </div>

            <button type="submit">Submit</button>
        </form>

        <div id="response" style="margin-top: 20px;">
            <h2>Results:</h2>
            <div id="streamOutput"></div>
        </div>
        <div class="loading-overlay" id="loadingOverlay">
            <div id="loading">Generation...</div>
        </div>
    </div>

    <script>
        // Toggle the model dropdown based on the selected provider
        function toggleModelDropdown() {
            const provider = document.getElementById('provider').value;
            const modelSelection = document.getElementById('modelSelection');
            modelSelection.style.display = provider === 'ollama' ? 'block' : 'none';
        }

        // Handle form submission
        document.getElementById('imageForm').onsubmit = async (event) => {
            event.preventDefault();

            // Collect form data
            const formData = {
                path: document.getElementById('path').value,
                provider: document.getElementById('provider').value,
                model: document.getElementById('provider').value === 'ollama'
                    ? document.getElementById('model').value
                    : '',
                useCache: document.getElementById('useCache').checked ? true : false
            };

            // Select the response container
            const streamOutput = document.getElementById('streamOutput');
            document.getElementById('loadingOverlay').style.display = 'flex';
            streamOutput.innerHTML = ''; // Clear previous results

            try {
                const response = await fetch('/process-images', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(formData)
                });

                // Read the streaming response
                const reader = response.body.getReader();
                const decoder = new TextDecoder("utf-8");

                let done = false;
                while (!done) {
                    const { value, done: streamDone } = await reader.read();
                    done = streamDone;

                    if (value) {
                        const chunk = decoder.decode(value, { stream: true });
                        // Append the received chunk to the output
                        document.getElementById('streamOutput').innerHTML += chunk;
                    }
                }

                document.getElementById('loadingOverlay').style.display = 'none';
            } catch (err) {
                // Handle errors
                console.error('Error:', err);
                streamOutput.innerHTML = `<p style="color: red;">An unexpected error occurred: ${err.message}</p>`;
            }
        };
    </script>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/refactor.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Documentation</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
</head>

<body>
    <div class="container">
        <h1>Generate Project Documentation</h1>

        <form id="documentationForm" method="POST">
            <label for="file">Choose a file:</label>
            <input type="file" id="file" name="file" required>

            <label for="gpt_provider">GPT Provider:</label>
            <select id="gpt_provider" name="gpt_provider" required onchange="toggleModelDropdown()">
                <option value="ollama">Ollama</option>
                <option value="gemini">Gemini</option>
            </select>

            <div id="modelSelection" style="display: none;">
                <label for="model">Model:</label>
                <select id="model" name="model">
                    {% for model in models %}
                    <option value="{{ model }}">{{ model }}</option>
                    {% endfor %}
                </select>
            </div>
            <button type="submit" id="generateDocumentationButton">Generate Documentation</button>
        </form>

        <!-- Documentation Content Section -->
        <div id="documentationContent" style="display: none;">
            <h2>Generated Documentation:</h2>
            <div id="documentationOutput">
                <!-- Streamed content will be added here -->
            </div>
        </div>

        <!-- Loading Overlay -->
        <div class="loading-overlay" id="loadingOverlay">
            <div id="loading">Generation...</div>
        </div>
        <br>
        <a href="{{ url_for('home_routes.home') }}" style="text-align: center; display: block;">Back to Dashboard</a>
    </div>

    <script>
        window.addEventListener('DOMContentLoaded', function () {
            // Load the remembered values from localStorage if available

            const savedGptProvider = localStorage.getItem('gpt_provider');

            const savedModel = localStorage.getItem('model');

            if (savedGptProvider) {
                document.getElementById('gpt_provider').value = savedGptProvider;
                // To show/hide model dropdown based on the provider
            }
            if (savedModel) {
                document.getElementById('model').value = savedModel;
            }

            toggleModelDropdown();
        });

        // Display loading overlay and handle form submission
        document.getElementById('documentationForm').addEventListener('submit', function (event) {
            event.preventDefault();
            // Save the form values to localStorage        
            localStorage.setItem('gpt_provider', document.getElementById('gpt_provider').value);
            localStorage.setItem('model', document.getElementById('model').value);

            document.getElementById('loadingOverlay').style.display = 'flex';
            document.getElementById('documentationOutput').innerHTML = '';

            // Fetch streaming data from the server
            console.log('this.action: ', this.action);
            fetch(this.action, {
                method: 'POST',
                body: new FormData(this),
            })
                .then(response => {
                    // Ensure response body can be read as text for streaming
                    const reader = response.body.getReader();
                    const decoder = new TextDecoder("utf-8");

                    // Function to process each chunk of streamed data
                    function readStream() {
                        return reader.read().then(({ done, value }) => {
                            if (done) {
                                document.getElementById('loadingOverlay').style.display = 'none';
                                return
                            }

                            document.getElementById('documentationContent').style.display = 'block';

                            document.getElementById('loadingOverlay').style.display = 'none';

                            // Append each streamed chunk to the documentation output
                            const chunk = decoder.decode(value, { stream: true });
                            const div = document.createElement('div');
                            if (!chunk.includes("id='agent_coder'")) {
                                div.classList.add('markdown-content');  // Add a class for styling (optional)
                            }
                            div.innerHTML = chunk;  // Set the chunk content
                            // Append the div containing the chunk to the markdown content
                            document.getElementById('documentationOutput').appendChild(div);
                            document.getElementById('loadingOverlay').style.display = 'flex';
                            document.getElementById('loadingOverlay').scrollIntoView({ behavior: "smooth", block: "end", inline: "nearest" });
                            return readStream();
                        });
                    }
                    return readStream();
                })
                .catch(error => {
                    document.getElementById('documentationOutput').innerHTML = `<p>Error: ${error.message}</p>`;
                    document.getElementById('loadingOverlay').style.display = 'none';
                });
        });

        function toggleModelDropdown() {
            const provider = document.getElementById('gpt_provider').value;
            console.log('provider: ', provider);
            document.getElementById('modelSelection').style.display = provider === 'ollama' ? 'block' : 'none';
        }
        // Show the loading indicator
        function showLoading() {
            document.getElementById('loading').style.display = 'block';  // Show loading indicator
            document.getElementById('response').style.display = 'none';  // Hide response during loading
        }
    </script>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/templates/split_file.html:

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Split File by Method</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
    <script>
        function toggleSplitValueInput() {
            const splitMethod = document.getElementById('split_method').value;
            const splitValueInput = document.getElementById('split_value_input');

            // Show the input only for "Split by Text" or "Split by Line Number"
            if (splitMethod === 'text' || splitMethod === 'lines') {
                splitValueInput.style.display = 'block';
            } else {
                splitValueInput.style.display = 'none';
            }
        }

        // Call toggleSplitValueInput when the page loads to ensure correct input is shown
        window.onload = function () {
            toggleSplitValueInput();
        }
    </script>
</head>

<body>
    <div class="container">
        <h1>Upload File and Choose Split Method</h1>

        <form method="POST" enctype="multipart/form-data">
            <label for="file">Choose a file (PDF, DOCX, TXT):</label>
            <input type="file" id="file" name="file" accept=".pdf,.docx,.txt" required>

            <label for="split_method">Choose a splitting method:</label>
            <select id="split_method" name="split_method" onchange="toggleSplitValueInput()" required>
                <option value="text">Split by Text</option>
                <option value="lines">Split by Line Number</option>
                <option value="paragraphs">Split by Paragraphs</option>
            </select>

            <div id="split_value_input" style="display:none;">
                <label for="split_value">Enter text or line number to split by:</label>
                <input type="text" id="split_value" name="split_value" placeholder="Text or line number">
            </div>

            <button type="submit">Split File</button>
        </form>

        <br>
        <a href="{{ url_for('home_routes.home') }}">Back to Dashboard</a>
    </div>
</body>

</html>
/home/cleudeir/Pessoal/easy_joby_py/src/routes/home.py:

from flask import Blueprint, render_template

home_routes = Blueprint('home_routes', __name__)

@home_routes.route('/')
def home():
    """Home Page"""
    return render_template('index.html')

/home/cleudeir/Pessoal/easy_joby_py/src/routes/directory_structure.py:

from flask import Blueprint, render_template, request
from src.modules.directory_structure import get_directory_structure

directory_structure_routes = Blueprint("directory_structure_routes", __name__)


@directory_structure_routes.route("/get-directory-structure", methods=["GET", "POST"])
def get_directory_structure_route():
    if request.method == "POST":
        directory_path = request.form["directory_path"]
        structure, content = get_directory_structure(directory_path)
        html = f"{structure}</br>\n\n{content}"
        return render_template("directory_structure.html", structure=html)
    return render_template("directory_structure.html")

/home/cleudeir/Pessoal/easy_joby_py/src/routes/file_splitter.py:

from flask import Blueprint, render_template, request
from src.modules.file_processor import (
    read_pdf, read_docx, read_txt,
    split_file_by_text, split_file_by_lines, split_file_by_paragraphs
)

file_splitter_routes = Blueprint('file_splitter_routes', __name__)

@file_splitter_routes.route('/file-splitter', methods=['GET', 'POST'])
def file_splitter():
    if request.method == 'POST':
        file = request.files['file']
        split_method = request.form['split_method']
        split_value = request.form.get('split_value', '')

        if file.filename.endswith('.pdf'):
            file_content = read_pdf(file)
        elif file.filename.endswith('.docx'):
            file_content = read_docx(file)
        elif file.filename.endswith('.txt'):
            file_content = read_txt(file)
        else:
            return "Unsupported file type", 400

        if split_method == 'text':
            if not split_value:
                return "Please provide text to split by.", 400
            sections = split_file_by_text(file_content, split_value)
            split_text = f"text '{split_value}'"
        elif split_method == 'lines':
            if not split_value.isdigit():
                return "Please provide a valid number of lines to split by.", 400
            sections = split_file_by_lines(file_content, int(split_value))
            split_text = f"{split_value} lines"
        elif split_method == 'paragraphs':
            sections = split_file_by_paragraphs(file_content)
            split_text = "paragraphs"
        else:
            return "Invalid split method", 400

        # Pass split_text to the template
        return render_template('split_results.html', sections=sections, split_text=split_text, split_method=split_method)
    
    return render_template('split_file.html')
/home/cleudeir/Pessoal/easy_joby_py/src/routes/agent_summary_reconstruction.py:

import os
from flask import Blueprint, current_app, render_template, request, Response
from src.modules.agent_summary_reconstruction_code import (
    get_agent_coder,
    get_agent_fix_summary,
    get_agent_summary,
    get_agent_improvement,
)
from src.modules.gpt import get_ollama_models
import markdown
import time

agent_summary_reconstruction_routes = Blueprint('agent_summary_reconstruction_routes', __name__)

@agent_summary_reconstruction_routes.route('/agent_summary_reconstruction_code', methods=['GET', 'POST'])
def agent_summary_reconstruction_code():
    available_models = get_ollama_models()
    documentation_html = None

    if request.method == 'POST':
        selected_model = request.form.get('model', '').strip()
        gpt_provider = request.form.get('gpt_provider', '').strip()
        file = request.files['file']
        file_content = None

        try:
            file_content = file.read().decode('utf-8')
        except Exception as e:
            documentation_html = f"<p>Error: Não foi possível ler o arquivo ({str(e)})</p>"
            return Response(documentation_html, mimetype='text/html')

        try:
            def generate_documentation():
                yield "<p>Starting documentation generation...</p>\n"
                time.sleep(0.100)
                yield markdown.markdown(f"<pre><code id='agent_coder'>{file_content.replace('<', '&lt;')}</code></pre>")
                time.sleep(0.100)
                
                summary = 'testes'
                code = 'testes'
                
                summary = get_agent_summary(gpt_provider, selected_model, file_content)
                yield markdown.markdown(summary)

                code = get_agent_coder(gpt_provider, selected_model, summary)
                yield f"<pre><code id='agent_coder'>{code.replace('<', '&lt;')}</code></pre>"

                agent_improvement = get_agent_improvement(gpt_provider, selected_model, file_content, summary)
                yield markdown.markdown(agent_improvement)

                summary = get_agent_fix_summary(gpt_provider, selected_model, file_content, summary, agent_improvement)
                yield markdown.markdown(summary)

                code = get_agent_coder(gpt_provider, selected_model, summary)
                yield f"<pre><code id='agent_coder'>{code.replace('<', '&lt;')}</code></pre>"

                time.sleep(0.100)
                yield "<p>Summary generation complete</p>\n"
                
                file_name, file_extension = f"{file.filename}".split('.')
                root_path = os.path.dirname(os.path.abspath(__file__)).split('src')[0]
                print(root_path)
                save_path = os.path.join(root_path, 'src/.outputs/reconstruction_code/')          
                os.makedirs(save_path, exist_ok=True)

                file_resume = os.path.join(save_path, file_name + '.md')
              
                with open(file_resume, 'w') as f:
                    f.write(summary)


                file_code_reconstruction = os.path.join(save_path, file_name + file_extension)
              
                with open(file_code_reconstruction, 'w') as f:
                    f.write(code)
    

                 

            return Response(generate_documentation(), mimetype='text/html')

        except Exception as e:
            error_message = f"Error generating documentation: {str(e)}"
            documentation_html = f"<p>{error_message}</p>"
            return render_template('agent_summary_reconstruction_code.html', documentation_html=documentation_html, models=available_models)

    return render_template('agent_summary_reconstruction_code.html', documentation_html=documentation_html, models=available_models)

/home/cleudeir/Pessoal/easy_joby_py/src/routes/refactor.py:

from flask import Blueprint, render_template, request, Response
from src.modules.refactor import get_agent_separate, get_agent_similarity
from src.modules.gpt import get_ollama_models
import markdown
import time

refactor_routes = Blueprint('refactor_routes', __name__)

@refactor_routes.route('/refactor', methods=['GET', 'POST'])
def refactor():
    available_models = get_ollama_models()
    documentation_html = None

    if request.method == 'POST':
        selected_model = request.form.get('model', '').strip()
        gpt_provider = request.form.get('gpt_provider', '').strip()
        file = request.files['file']
        file_content = None
        
        try:
            file_content = file.read().decode('utf-8')         
        except Exception as e:
            documentation_html = f"<p>Error: Não foi possível ler o arquivo ({str(e)})</p>"            
            return Response(documentation_html, mimetype='text/html')
        
        try:
            # Generate documentation in chunks
            def generate_documentation():
                yield "<p>Starting documentation generation...</p>\n"
                time.sleep(0.100)
                yield markdown.markdown(f"<pre><code id='agent_coder'>{file_content.replace('<', '&lt;')}</code></pre>")
                time.sleep(0.100)
                
                coder = get_agent_separate(gpt_provider, selected_model, file_content)
                yield markdown.markdown(f"<pre><code id='agent_coder'>{coder.replace('<', '&lt;')}</code></pre>")
                
                agent_evaluation = get_agent_similarity(file_content, coder)
                yield markdown.markdown(f'Agent evaluation: {agent_evaluation}')
                
                time.sleep(0.100)
                yield "<p>Summary generation complete</p>\n" 
                print("Documentation generation complete.")
            
            return Response(generate_documentation(), mimetype='text/html')

        except Exception as e:
            error_message = f"Error generating documentation: {str(e)}"
            documentation_html = f"<p>{error_message}</p>"
            return render_template('agent_summary_reconstruction_code.html', documentation_html=documentation_html, models=available_models)
    
    return render_template('agent_summary_reconstruction_code.html', documentation_html=documentation_html, models=available_models)


/home/cleudeir/Pessoal/easy_joby_py/src/routes/project_documentation.py:

import os
import fnmatch
import time
from flask import Blueprint, current_app, render_template, request, Response
from src.modules.project_documentation import (
    get_project_files,
    read_and_summarize_file,
    get_general_summary,
)
from src.modules.directory_structure import get_directory_structure
from src.modules.gpt import get_ollama_models
import markdown

project_documentation_routes = Blueprint("project_documentation_routes", __name__)


@project_documentation_routes.route(
    "/get_project_documentation", methods=["GET", "POST"]
)
def get_project_documentation():
    available_models = get_ollama_models()
    documentation_html = None

    if request.method == "POST":
        project_path = request.form.get("project_path", "").strip()
        selected_model = request.form.get("model", "").strip()
        gpt_provider = request.form.get("gpt_provider", "").strip()
        uploads_dir = os.path.join(current_app.root_path, "src/.outputs" + project_path)

        if not project_path:
            documentation_html = "<p>Error: Project directory path is required.</p>"
            return render_template(
                "project_documentation.html",
                documentation_html=documentation_html,
                models=available_models,
            )

        os.makedirs(uploads_dir, exist_ok=True)

        try:

            def generate_documentation():
                yield "<p>Starting documentation generation...</p>\n"
                ignore_patterns = ["project_documentation.txt"]
                combined_summary = ""

                for file_path in get_project_files(project_path):
                    file_name = os.path.basename(file_path)
                    if any(
                        fnmatch.fnmatch(file_name, pattern)
                        for pattern in ignore_patterns
                    ):
                        continue

                    summary = read_and_summarize_file(
                        file_path, gpt_provider, selected_model, uploads_dir
                    )
                    combined_summary += summary
                    yield markdown.markdown(summary) + "\n"

                # GENERATE SUMMARY
                structure, _ = get_directory_structure(project_path)
                general_summary = get_general_summary(
                    summary=combined_summary,
                    gpt_provider=gpt_provider,
                    model=selected_model,
                )
                general_summary_file = f"""
                # {file_name}                
                ## project structure
                ```                    
                {structure}                
                ```
                {general_summary}                
                """
                # Save the summary to a file
                with open(os.path.join(uploads_dir, "README.md"), "w") as f:
                    f.write(general_summary_file)
                # code extension code form
                general_summary_html = f"<h1>README.md</h1>\n"
                general_summary_html += f"<h2>Project structure</h2>\n"
                general_summary_html += f"<pre><code>{structure}</code></pre>\n"
                general_summary_html += markdown.markdown(general_summary)
                yield general_summary_html
                time.sleep(0.100)

                yield "<p>Documentation generation complete.</p>\n"
                print("Documentation generation complete.")

            return Response(generate_documentation(), mimetype="text/html")

        except Exception as e:
            error_message = f"Error generating documentation: {str(e)}"
            documentation_html = f"<p>{error_message}</p>"
            return render_template(
                "project_documentation.html",
                documentation_html=documentation_html,
                models=available_models,
            )

    return render_template(
        "project_documentation.html",
        documentation_html=documentation_html,
        models=available_models,
    )

/home/cleudeir/Pessoal/easy_joby_py/src/routes/image_description.py:

import io
import os
import shutil
import time
from PIL import Image
from flask import Blueprint, render_template, request, current_app, Response, jsonify, stream_with_context
from src.modules.gpt import  get_ollama_vision_models
from src.modules.image_description import describe_image_with_ollama, get_images_from_path, describe_image_with_gemini
import markdown

image_description_routes = Blueprint('image_description_routes', __name__)

@image_description_routes.route("/process-images", methods=["POST", "GET"])
def image_description():
    available_models = get_ollama_vision_models()
    if request.method == "GET":
        return render_template("image_description.html", models=available_models)

    elif request.method == "POST":
        data = request.json
        input_path = data.get("path")
        selected_model = data.get('model', '').strip()
        gpt_provider = data.get('provider', '').strip()
        useCache = data.get("useCache", False)
        
        output_path = os.path.join(current_app.root_path, 'src/.outputs' + input_path)
        static_image_path = os.path.join(current_app.root_path, 'src/static/images')
        os.makedirs(output_path, exist_ok=True)
        os.makedirs(static_image_path, exist_ok=True)
    try:
        # Get all images in the directory
        images = get_images_from_path(input_path)
       
        def render():
            if(len(images) == 0):
                yield "<p>No images found in the directory.</p>\n"
                return
            yield "<p>Starting documentation generation...</p>\n"
            docx_html = ''
          
            for img in images:
                path_to_copy = os.path.join(static_image_path, os.path.basename(img))
                shutil.copyfile(img, path_to_copy)
                
                url_image = os.path.join('static/images', os.path.basename(img))
                print(f"Image saved: {url_image}")
                # get dimensions of the image
                with open(img, 'rb') as f:
                    image_data = f.read()
                    image = Image.open(io.BytesIO(image_data))
                    width, height = image.size
                    proportions = width/height
                    print(f"Image proportions: {proportions}")
                    if(proportions > 1):
                        docx_html += f"<img src='{img}' alt='{os.path.basename(img)}'width='{650}px' height='{650 / proportions }px'/>"
                    else:
                        docx_html += f"<img src='{img}' alt='{os.path.basename(img)}'width='{800 * proportions}px' height='{800}px'/>"
                    
                html_image = f"<img src='{url_image}' alt='{os.path.basename(url_image)}' style='max-width: 100%; height: auto;' />"
          
                
                yield html_image
                
                document_html = os.path.join(output_path, os.path.basename(img).split('.')[0] + '.html')
                if os.path.exists(document_html) and useCache:                    
                    with open(document_html, 'r') as f:
                        content = f.read()
                        docx_html += content
                        yield content
                    continue
                markdown_text = ''
                if(gpt_provider == 'gemini'):
                    markdown_text = describe_image_with_gemini(img)
                elif(gpt_provider == 'ollama'):
                    markdown_text = describe_image_with_ollama(img, selected_model)
                html_text = markdown.markdown(markdown_text, extensions=['extra', 'tables'])
                docx_html += html_text
                file_html = document_html
                with open(file_html, 'w') as f:
                    f.write(html_text)
                yield html_text
                time.sleep(0.100)  
            docx_file = os.path.join(output_path, 'project_documentation.docx')
            with open(docx_file, 'wb') as f:
                f.write(docx_html.encode('utf-8'))
            # command linux open docx
            try:
                os.system(f"libreoffice {docx_file}")
            except:
                print("Libreoffice not found")
            
            time.sleep(0.100)            
            yield "<p>Documentation generation complete.</p>\n"
            
        return Response(stream_with_context(render()), mimetype='text/html')

    except Exception as e:
        return jsonify({"error": str(e)}), 500

/home/cleudeir/Pessoal/easy_joby_py/src/routes/ollama_response.py:

from flask import Blueprint, render_template, request
from src.modules.gpt import get_ollama_models, get_ollama_response

ollama_response_routes = Blueprint('ollama_response_routes', __name__)

@ollama_response_routes.route('/ollama-response', methods=['GET', 'POST'])
def ollama_response():
    models = get_ollama_models()
    if request.method == 'POST':
        model = request.form['model']
        system_prompt = request.form['system_prompt']
        user_prompt = request.form['user_prompt']

        response = get_ollama_response(model, system_prompt, user_prompt)
        return render_template(
            'ollama.html',
            response=response,
            model=model,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            models=models
        )
    return render_template('ollama.html', models=models)
/home/cleudeir/Pessoal/easy_joby_py/src/modules/concatenate_project.py:

import os
import uuid


def get_concatenate_project(directory_path):
    if not os.path.exists(directory_path):
        return "Directory does not exist."

    # Generate a unique file name for the concatenated file
    unique_file_name = f"concatenated_{uuid.uuid4().hex}.txt"
    unique_file_path = os.path.join(directory_path, unique_file_name)

    structure = []
    concatenated_content = []

    for dirpath, dirnames, filenames in os.walk(directory_path):
        # Ignore any directories that start with '.' or '__'
        dirnames[:] = [
            d for d in dirnames if not (d.startswith(".") or d.startswith("__"))
        ]

        # Calculate the level of indentation
        level = dirpath.replace(directory_path, "").count(os.sep)
        indent = " " * 4 * level
        structure.append(f"{indent}{os.path.basename(dirpath)}/")

        # Add files, ignoring those that start with '.' or '__'
        subindent = " " * 4 * (level + 1)
        for f in filenames:
            if not (f.startswith(".") or f.startswith("__")):
                structure.append(f"{subindent}{f}")

                # Read file content and append it to concatenated_content
                file_path = os.path.join(dirpath, f)
                try:
                    with open(file_path, "r", encoding="utf-8") as file:
                        concatenated_content.append(f"--- Start of {f} ---\n")
                        concatenated_content.append(file.read())
                        concatenated_content.append(f"\n--- End of {f} ---\n")
                except Exception as e:
                    concatenated_content.append(f"\n--- Failed to read {f}: {e} ---\n")

    # Write the concatenated content into the unique file
    try:
        with open(unique_file_path, "w", encoding="utf-8") as unique_file:
            unique_file.write("\n".join(concatenated_content))
    except Exception as e:
        return f"Failed to write concatenated file: {e}"

    # Read back the content of the unique file
    try:
        with open(unique_file_path, "r", encoding="utf-8") as unique_file:
            unique_file_content = unique_file.read()
    except Exception as e:
        return f"Failed to read concatenated file: {e}"

    # Return the directory structure and the unique file content
    return f"Directory structure:\n\n{chr(10).join(structure)}\n\nContent of the concatenated file:\n\n{unique_file_content}"

/home/cleudeir/Pessoal/easy_joby_py/src/modules/agent_summary_reconstruction_code.py:


import re
from src.modules.gpt import get_ollama_embeddings, get_ollama_response, get_genai_response
import json
from sklearn.metrics.pairwise import cosine_similarity

structure = """
    # File Summary
    ## Language Used and Frameworks
    - What programming language is used in the file, and any frameworks?
    ---
    ## Purpose of the File
    - What is the primary purpose of this file?
    ---
    ## Functions and Classes Defined
    - List each function or class (name).
        - Name
            - Input
            - Output
    ---
    ## Execution Flow
    - What is the hierarchical execution flow of the file?
    ---    
    ## Variables and Constants
    - List each variable or constant along with its purpose and its value.
    ---
    ## Input and Output
    - What are the inputs and outputs of the file?
    ## InternalDependencies
    - Name modules are imported?
    ## External Dependencies
    - What name libraries are imported?
    ---
    """  

def get_agent_summary(gpt_provider, model, file_content):  
    
    system_prompt  = "You are a software engineer. will create documentation."     

    user_prompt = f"""
    {file_content}   
    Summarize that code.
    Use this structure:
    {structure}
    not use code in the response, not use exemple, no comments, no explanation.  
    """
    # Use Ollama to generate the summary with the specified model
    if(gpt_provider == "ollama"):
        data = get_ollama_response(model, system_prompt, user_prompt)
    elif(gpt_provider == "gemini"):
        data = get_genai_response(system_prompt=system_prompt, user_prompt=user_prompt)
    # Check if the response is a dictionary, which indicates an error
    if isinstance(data, dict) and "error" in data:
        return f"**Error**: {data['error']}\n"
    # Return the formatted summary
    return data

def get_agent_coder(gpt_provider, model, summary):   
    system_prompt  = "You are a software engineer."
    summary = f"""
    create code using that summary of the code:
    {summary}
    only create code, no comments in code, no explanation, only code. create with perfect indentation, create complete code.
    """
    # Use Ollama to generate the summary with the specified model
    if(gpt_provider == "ollama"):
        data = get_ollama_response(model, system_prompt, summary)
    elif(gpt_provider == "gemini"):
        data = get_genai_response(system_prompt=system_prompt, user_prompt=summary)
    print(data)
    # Check if the response is a dictionary, which indicates an error
    if isinstance(data, dict) and "error" in data:
        return f"**Error**: {data['error']}\n"
    # Return the formatted summary
    # Regular expression to match the content inside the curly braces {}
    regex = r"```[a-zA-Z]+\n([\s\S]*?)```"
    # Find all matches
    matches = re.findall(regex, data)
    # Extract and print only the content inside the curly braces
    for content in matches:
        # Removing leading and trailing spaces
        return content.strip()

def get_agent_similarity(file_content, reconstruct_code):
    embeddings_code = get_ollama_embeddings(model_name='nomic-embed-text:latest', text_input=file_content)
    embeddings_reconstruct_code = get_ollama_embeddings(model_name='nomic-embed-text:latest', text_input=reconstruct_code)
    similaridade = cosine_similarity(embeddings_code, embeddings_reconstruct_code)
    return similaridade[0][0]

def get_agent_improvement(gpt_provider, model, file_content, summary):       
    system_prompt  = "You are a software engineer. will create documentation."
    user_prompt =f"""    
    That is code: 
    {file_content}
    and this is a summary of the code:
    {summary}
    Analyze this summary and appoint the missing and wrong points.
    Response in this structure:
    
    ### Missing:
    * appoint in bullet points.
    ### Wrong:
    * appoint in bullet points.  
      
    flow structure, no comments, no explanation.
    """
    # Use Ollama or Gemini to generate the summary with the specified model
    if gpt_provider == "ollama":
        data = get_ollama_response(model, system_prompt, user_prompt)
    elif gpt_provider == "gemini":
        data = get_genai_response(system_prompt=system_prompt, user_prompt=user_prompt)
    
    if isinstance(data, dict) and "error" in data:
        return f"**Error**: {data['error']}\n"
    
    return data

def get_agent_fix_summary(gpt_provider, model, file_content, summary, improvement):  
    system_prompt = "You are a software engineer.  will create documentation"
    user_prompt = f"""  
    Attention in this appointment:
    {improvement}
    That is a code:
    {file_content}
    Create a summary of the code using this structure:
    {structure}
    not use code in the response, not use exemple, no comments, no explanation.    
    """
    # Use Ollama to generate the summary with the specified model
    if(gpt_provider == "ollama"):
        data = get_ollama_response(model, system_prompt, user_prompt)
    elif(gpt_provider == "gemini"):
        data = get_genai_response(system_prompt=system_prompt,user_prompt=user_prompt)
    # Check if the response is a dictionary, which indicates an error
    if isinstance(data, dict) and "error" in data:
        return f"**Error**: {data['error']}\n"
    # Return the formatted summary
    return data
/home/cleudeir/Pessoal/easy_joby_py/src/modules/directory_structure.py:

import os


def get_directory_structure(directory_path):
    if not os.path.exists(directory_path):
        return "Directory does not exist."

    structure = []
    file_contents = []

    for dirpath, dirnames, filenames in os.walk(directory_path):
        # Ignore any directories that start with '.' or '__'
        dirnames[:] = [
            d
            for d in dirnames
            if not (d.startswith(".") or d.startswith("__") or d in ["bin", "obj"])
        ]

        # Calculate the level of indentation
        level = dirpath.replace(directory_path, "").count(os.sep)
        indent = " " * 4 * level
        structure.append(f"{indent}{os.path.basename(dirpath)}/")

        # Add files, ignoring those that start with '.' or '__'
        subindent = " " * 4 * (level + 1)
        for f in filenames:
            if not (f.startswith(".") or f.startswith("__")):
                file_path = os.path.join(dirpath, f)
                structure.append(f"{subindent}{f}")
                try:
                    with open(file_path, "r", encoding="utf-8") as file:
                        content = file.read()
                        file_contents.append(f"{file_path}:\n\n{content}")
                except Exception as e:
                    file_contents.append(f"{file_path}:\n[Error reading file: {e}]")

    # Combine structure and contents
    return ["\n".join(structure), "\n".join(file_contents)]

/home/cleudeir/Pessoal/easy_joby_py/src/modules/refactor.py:


import re
from src.modules.gpt import get_ollama_embeddings, get_ollama_response, get_genai_response
from sklearn.metrics.pairwise import cosine_similarity

def get_agent_separate(gpt_provider, model, file_content):  
    system_prompt  = "You are a software engineer"    
    user_prompt = f"""
    {file_content}
    refactor this code, separate in functions, only create code, no comments in code, no explanation, only code. create with perfect indentation, create complete code.
    """
    # Use Ollama to generate the summary with the specified model
    if(gpt_provider == "ollama"):
        data = get_ollama_response(model, system_prompt, user_prompt)
    elif(gpt_provider == "gemini"):
        data = get_genai_response(system_prompt=system_prompt, user_prompt=user_prompt)
    # Check if the response is a dictionary, which indicates an error
    if isinstance(data, dict) and "error" in data:
        return f"**Error**: {data['error']}\n"
    # Return the formatted summary
    return data

def get_agent_similarity(file_content, reconstruct_code):
    embeddings_code = get_ollama_embeddings(model_name='nomic-embed-text:latest', text_input=file_content)
    embeddings_reconstruct_code = get_ollama_embeddings(model_name='nomic-embed-text:latest', text_input=reconstruct_code)
    similaridade = cosine_similarity(embeddings_code, embeddings_reconstruct_code)
    return similaridade[0][0]
/home/cleudeir/Pessoal/easy_joby_py/src/modules/gpt.py:

import os
import ollama
import google.generativeai as genai  # M

# Function to get a response from Google Generative AI with model, system, and user prompts
def get_genai_response(system_prompt, user_prompt):
    try:   
        model_name = "gemini-1.5-flash"
        # Configure the API key
        genai.configure(api_key=os.environ["GEMINI_API_KEY"])
        # Define the generation configuration
        generation_config = {
            "temperature": 1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
            "response_mime_type": "text/plain",
        }
        
        # Create the generative model with the specified configuration and system instruction
        model = genai.GenerativeModel(
            model_name=model_name,
            generation_config=generation_config,
            system_instruction=system_prompt,
        )
        
        # Start a chat session
        chat_session = model.start_chat(history=[])
        
        # Send the user prompt and return the response
        response = chat_session.send_message(user_prompt)
        return response.text
    except Exception as e:
        return {"error": str(e)}


# Function to get available models from Ollama
def get_ollama_models():
    try:
        # Fetch available models using ollama.list
        response = ollama.list()
        models = response.get('models', [])  # Extract models from the response
        model_names = [model['name'] for model in models]  # Get the model names
        return model_names
    except Exception as e:
        return {"error": str(e)}
    
def get_ollama_vision_models():
    try:
        # Fetch available models using ollama.list
        response = ollama.list()
        models = response.get('models', [])  # Extract models from the response
        # Filter models with name vision or llava
        models = [model for model in models if 'vision' in model['name'].lower() or 'llava' in model['name'].lower()]
        # return only names
        models = [model['name'] for model in models]
        
        return models
    except Exception as e:
        return {"error": str(e)}

# Function to get a response from an Ollama model
def get_ollama_response(model_name, system_prompt, user_prompt):
    try:
        messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]        
        # Call ollama.chat with the specified model and messages
        response = ollama.chat(model=model_name, messages=messages)
        # Return the response content directly
        response_message = response['message']['content']
        return response_message
    except Exception as e:
        return {"error": str(e)}

# Function to get embeddings from an Ollama model
def get_ollama_embeddings(model_name, text_input):
    try:
        # Call ollama.embedding with the specified model and input text
        response = ollama.embed(model=model_name, input=text_input)
        # Extract and return the embeddings
        embeddings = response['embeddings']
        return embeddings
    except Exception as e:
        return {"error": str(e)}


/home/cleudeir/Pessoal/easy_joby_py/src/modules/project_documentation.py:

import os
import time
from src.modules.file_processor import read_pdf, read_docx
from src.modules.gpt import get_ollama_response, get_genai_response
from flask import current_app


def get_summary(content, filename, gpt_provider, model):
    system_prompt = """
    you are a software engineer creating a project documentation.  
    """
    user_prompt = f"""
    This is a code:
    {content}
    Flow structure and summary, no comments, no explanation:
    ## Summary: 
        * {filename}
    ##  Project purpose and description
        * ...
    ## Business rule
        * ...
    ## how is Pipeline
        * ...
    ---
    Create summary without create code.
    """

    # Use Ollama to generate the summary with the specified model
    if gpt_provider == "ollama":
        summary = get_ollama_response(model, system_prompt, user_prompt)
    elif gpt_provider == "gemini":
        summary = get_genai_response(
            system_prompt=system_prompt, user_prompt=user_prompt
        )
    # Check if the response is a dictionary, which indicates an error
    if isinstance(summary, dict) and "error" in summary:
        return f"**Error**: {summary['error']}\n"
    # Return the formatted summary
    return summary


def get_general_summary(summary, gpt_provider, model):
    system_prompt = """
    You are a software engineer creating a README.md for GitHub.
    """
    user_prompt = f"""
    That is summary:
    {summary}
    Follow this structure to create a summary:
    ## Project purpose and description
        * ... 
    ## Dependencies
        * Before you can start using or working with this project, make sure to install the following dependencies:
        ```
        * Dependencies name
        ``
    ## How to Install
        * To get this project up and running, follow these steps:
            * **Clone this repository;
            * **Install dependencies:** `...`;
            * **Create a .env file:** `...`;
            * **Run the application:** `...`;            
    ## How to Use
        *  ...
    ## how is architecture
        *  ...
    ## how is pipeline
        *  ...
    
    Create general summary of this project, no comments, no explanation, responda em Portuguese.
    Create summary without create code.
    """

    if gpt_provider == "ollama":
        summary = get_ollama_response(model, system_prompt, user_prompt)
    elif gpt_provider == "gemini":
        summary = get_genai_response(system_prompt, user_prompt)

    if isinstance(summary, dict) and "error" in summary:
        return f"**Error**: {summary['error']}\n"

    return summary


def get_project_files(directory):
    """
    Recursively gets all files in the provided directory, excluding paths that start with ".".
    """
    project_files = []
    for root, dirs, files in os.walk(directory):
        # Skip directories that start with "."
        dirs[:] = [d for d in dirs if not (d.startswith(".") or d.startswith("__"))]

        for file in files:
            # Only add files with specific extensions and ignore those starting with "."
            if not file.startswith(".") and file.endswith(
                (
                    ".py",
                    ".txt",
                    ".html",
                    ".yml",
                    ".js",
                    ".ts",
                    ".tsx",
                    ".jsx",
                    ".json",
                    "java",
                )
            ):
                project_files.append(os.path.join(root, file))
    return project_files


def read_and_summarize_file(filepath, gpt_provider, model, uploads_dir):
    try:
        """
        Reads a file and returns its summary using the specified model,
        saving the summary in the uploads directory with the same structure as the original file.
        """
        # start time
        start_time = time.time()
        filename = os.path.basename(filepath)

        split_path = uploads_dir.split("/")[-1]
        join_path = uploads_dir + filepath.split(split_path)[1]
        file_name_only = (join_path.split("/")[-1]).split(".")[0]
        join_path = "/".join(join_path.split("/")[:-1]) + "/" + file_name_only + ".txt"

        # check if file already exists
        if os.path.exists(join_path):
            # read file and return
            with open(join_path, "r") as file:
                return file.read()

        # Determine the relative path within the directory structure and create it in uploads
        os.makedirs(os.path.dirname(uploads_dir), exist_ok=True)

        # Read file content based on the file type
        content = None
        if filepath.endswith(".txt"):
            with open(filepath, "r") as file:
                content = file.read()
        elif filepath.endswith(".pdf"):
            with open(filepath, "rb") as file:
                content = read_pdf(file)
        elif filepath.endswith(".docx"):
            with open(filepath, "rb") as file:
                content = read_docx(file)
        else:
            with open(filepath, "r") as file:
                content = file.read()

        if gpt_provider == "gemini":
            time.sleep(4)

        # Generate the summary using the specified model
        summary = get_summary(content, filename, gpt_provider, model)
        # Save the summary to the uploads directory

        os.makedirs(os.path.dirname(join_path), exist_ok=True)
        with open(join_path, "w") as summary_file:
            summary_file.write(summary)

        # delay time around
        end_time = time.time()
        print(
            "\nTime taken to generate summary: ",
            int((end_time - start_time) * 1000),
            "ms\n",
        )

        return summary  # Optionally return the summary if needed elsewhere
    except Exception as e:
        return {"error": str(e)}

/home/cleudeir/Pessoal/easy_joby_py/src/modules/image_description.py:

import os
import ollama
import google.generativeai as genai
from src.modules.gpt import get_genai_response, get_ollama_response  # M

def get_images_from_path(path: str):
    """
    Reads a directory and returns a list of image file paths.
    """
    supported_formats = {".jpg", ".jpeg", ".png", ".bmp", ".gif"}
    return [
        os.path.join(path, file)
        for file in os.listdir(path)
        if os.path.isfile(os.path.join(path, file)) and os.path.splitext(file)[1].lower() in supported_formats
    ]


prompt_image_documents = """
    Analise as Imagens e Ajude a Documentar o Software, flow structure, no comments, no explanation.
    
    ## Nome da tela
    * Nome
    
    ## Descrição da tela
    * breve descrição da tela
    
    (Se existir busca e/ou filtro)
    ## Campos input busca e/ou filtro:
    Formato de Tabela com campos com a estrutura:
    “Nome,Tipo,Validação,Observação”
    
    (Se existir cartão com indicadores)
        ## Cartões com Indicadores:
        Formato de Tabela com campos com a estrutura:
        “Nome,Tipo,Validação,Observação”
    
    Se existir gráfico
        ## Gráficos:
        Formato de Tabela com campos com a estrutura:
        “Nome,Tipo,Eixo X (nome e unidade),Eixo Y(nome e unidade),”
        Se existir rádios no gráfico, adicionar subtítulo "variações" listando em formato de lista todas as opções e explique a função..

    Se existir lista
        ## Lista:
        Formato de Tabela com campos com a estrutura:
        “Nome, Tipo, Validação, Observação”
        se a lista tiver como resultado  "Nenhum registro foi encontrado ...", adicione um texto: "O sistema não apresenta nenhum dado registrado, o que torna insuficientes as informações para a descrição deste item. Dados adicionais serão incluídos nos próximos relatórios de detalhamento."
        
    Se existir formulário
        ## Formulário : 
        ### nome do fomulário
        Formato de Tabela com campos com a estrutura:
        “Nome, Tipo, Validação, Observação”
        Se existem campos com o ícone de cadeado, adicione abaixo da tabela o subtitulo:         
            ### Observação :
            * Os campos com o ícone de cadeado permitem que o valor seja travado para reutilização em futuras transferências, facilitando o preenchimento para valores que são 
            frequentemente reutilizados."
    
    ## Regra de Negócio:
    * Lógica de Processamento: crie usando a imagem como referência,
    * Ações Condicionais: crie usando a imagem como referência,
    
    
    Regras criação do relatório:       
    - Verifique se exite elemento semelhante a estrutura se existir adicione no relatório.
    - Sempre crie a regra de negócio.
    - Não adicione comentários, apenas títulos e subtítulos quando necessário.
    - Não use traços ou linhas como separador entre os parágrafos.
    - se campo for vazio, adicione a informação: “-”
    - se o tipo de item não existir, não o adicione.
    - create using markdown format.
    - responda em português.
    """
    
def describe_image_with_ollama(image_path, model_name):
    """
    Uses Ollama Vision to describe an image.
    
    Args:
        model_name (str): The name of the Ollama model to use.
        image_path (str): Path to the image file.
        user_prompt (str): The user prompt for the description.

    Returns:
        str: The description provided by the Ollama model.
    """
   
    try:
        # Send the user message and image to the Ollama Vision model
        response = ollama.chat(
            model=model_name,
            messages=[{
                'role': 'user',
                'content': prompt_image_documents,
                'images': [image_path]
            }]
        )
        # Extract and return the content of the response
        return response.get('message', {}).get('content', 'No description available')
    except Exception as e:
        return f"Error describing image {image_path}: {str(e)}"
    
def describe_image_with_gemini(image_path):
    # Create the model
    generation_config = {
    "temperature": 1,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
    "response_mime_type": "text/plain",
    }

    model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    generation_config=generation_config,
    )
    
    file = genai.upload_file(image_path)
    chat_session = model.start_chat(
    history=[
        {
        "role": "user",
        "parts": [
            file,  
            prompt_image_documents          
        ],
        },       
    ]
    )
    response = chat_session.send_message('create')
    print(response.text)
    return response.text
/home/cleudeir/Pessoal/easy_joby_py/src/modules/file_processor.py:

import pdfplumber
import docx

def split_file_by_text(file_content, split_text):
    """
    Split the given file content by the provided text.
    :param file_content: The content of the file as a string
    :param split_text: The text to use as the splitting point
    :return: A list of strings, each representing a section of the split content
    """
    sections = file_content.split(split_text)
    return sections

def split_file_by_lines(file_content, lines_per_section):
    """
    Split the file content into sections based on the number of lines.
    :param file_content: The content of the file as a string
    :param lines_per_section: Number of lines per section
    :return: A list of strings, each representing a section of the split content
    """
    lines = file_content.splitlines()
    sections = [lines[i:i+lines_per_section] for i in range(0, len(lines), lines_per_section)]
    return ['\n'.join(section) for section in sections]

def split_file_by_paragraphs(file_content):
    """
    Split the file content into sections based on paragraphs.
    :param file_content: The content of the file as a string
    :return: A list of paragraphs
    """
    paragraphs = file_content.split('\n')
    return paragraphs

def read_pdf(file):
    """
    Extract text from a PDF file while preserving original formatting.
    :param file: File object (PDF)
    :return: The text content of the PDF as a string
    """
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            # Extract text from the page, preserving layout
            text += page.extract_text() + "\n"
    return text

def read_docx(file):
    """
    Extract text from a DOCX file.
    :param file: File object (DOCX)
    :return: The text content of the DOCX as a string
    """
    doc = docx.Document(file)
    return '\n'.join([para.text for para in doc.paragraphs])

def read_txt(file):
    """
    Extract text from a TXT file.
    :param file: File object (TXT)
    :return: The text content of the TXT as a string
    """
    return file.read().decode('utf-8')

/home/cleudeir/Pessoal/easy_joby_py/src/static/css/styles.css:

/* General Styles */
body {
    font-family: 'Arial', sans-serif;
    line-height: 1.6;
    background-color: #f4f4f9;
    margin: 0;
    padding: 20px;
    color: #333;

}

h1,
h2,
h3 {
    color: #333;
}

h1 {
    font-size: 2.5em;
    margin-bottom: 20px;
    text-align: center;
    color: #2c3e50;
}

h2 {
    font-size: 1.8em;
    margin: 20px 0;
    color: #34495e;
}

/* Container for centering content */
.container {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background: #ffffff;
    border-radius: 8px;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

/* Links */
a {
    color: #3498db;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

/* Form Styles */
form {
    background-color: #fff;
    border: 1px solid #dfe6e9;
    border-radius: 6px;
    padding: 20px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
    margin-bottom: 20px;
}

label {
    font-weight: bold;
    display: block;
    margin-bottom: 5px;
    color: #2d3436;
}

/* Input and Textarea Styles */
textarea,
select,
input[type="text"],
select,
button {
    width: 95%;
    padding: 10px;
    margin-top: 8px;
    border: 1px solid #dfe6e9;
    border-radius: 6px;
    font-size: 1em;
    background-color: #f9f9f9;
}

select {
    width: 98%;
}

input[type="checkbox"] {
    margin-top: 25px;
    margin-bottom: 25px;
}

textarea {
    resize: vertical;
    width: calc(100% - 20px);
    /* Allows users to resize the textarea vertically */
    min-height: 120px;
    /* Ensure the textarea starts at a reasonable height */
}



select:focus {
    border-color: #3498db;
    outline: none;
}

/* Button Styles */
button {
    background-color: #3498db;
    color: white;
    border: none;
    cursor: pointer;
    font-size: 1em;
    font-weight: bold;
    transition: background-color 0.3s ease;
}

button:hover {
    background-color: #2980b9;
}

/* Preformatted Text Styles */
pre {
    background-color: #f4f4f9;
    border: 1px solid #dfe6e9;
    padding: 15px;
    overflow-x: auto;
    border-radius: 6px;
    white-space: pre-wrap;
    /* Allows long lines to break and wrap */
}

/* Add this to your existing CSS file for a simple loading spinner */
#loading::before {
    content: "";
    display: inline-block;
    width: 24px;
    height: 24px;
    border: 3px solid #3498db;
    border-top-color: transparent;
    border-radius: 50%;
    animation: spin 1s linear infinite;
    margin-right: 10px;
}

.loading-overlay {
    display: none;
    /* Hidden initially */
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    margin-top: 50px;
    margin-bottom: 50px;
    align-items: center;
    justify-content: center;
    font-size: 1.5em;
    color: #333;
}


/* Markdown Content Styling */
.markdown-content {
    background-color: #f9f9f9;
    border: 1px solid #dfe6e9;
    margin-top: 20px;
    padding: 20px;
    border-radius: 6px;
    overflow-x: auto;
}

.markdown-content h1,
.markdown-content h2,
.markdown-content h3 {
    color: #333;
}

.markdown-content pre {
    background-color: #f4f4f9;
    padding: 15px;
    border-radius: 6px;
    overflow-x: auto;
}



.markdown-content code {
    background-color: #e8e8e8;
    padding: 2px 4px;
    border-radius: 4px;
}

/* Table Styles */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
    font-size: 1em;
    background-color: #ffffff;
    border-radius: 6px;
    overflow: hidden;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

thead {
    background-color: #3498db;
    color: #ffffff;
    text-align: left;
}

thead th {
    padding: 12px 15px;
    font-weight: bold;
    text-transform: uppercase;
}

tbody tr {
    border-bottom: 1px solid #dfe6e9;
}

tbody tr:nth-of-type(even) {
    background-color: #f4f4f9;
}

tbody tr:last-of-type {
    border-bottom: 2px solid #3498db;
}

tbody td {
    padding: 12px 15px;
    color: #2c3e50;
    vertical-align: top;
    text-align: left;
}

/* Spinner animation */
@keyframes spin {
    to {
        transform: rotate(360deg);
    }
}

/* Responsive Styles */
@media (max-width: 768px) {
    h1 {
        font-size: 2em;
    }

    h2 {
        font-size: 1.5em;
    }

    textarea,
    select,
    input[type="text"],
    button {
        font-size: 0.9em;
    }

    button {
        padding: 12px;
    }
}

/* Responsive Table */
@media (max-width: 768px) {
    table {
        font-size: 0.9em;
    }

    thead th {
        font-size: 0.85em;
    }

    tbody td {
        font-size: 0.85em;
    }
}